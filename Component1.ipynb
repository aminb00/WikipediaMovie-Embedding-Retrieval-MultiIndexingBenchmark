{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f92488c",
   "metadata": {},
   "source": [
    "# Component 1: Document Embedding with Word2Vec\n",
    "\n",
    "This component transforms raw movie documents into dense vector representations using Word2Vec embeddings.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load movie datasets (1970s-2020s)\n",
    "2. Tokenize documents (title + plot)\n",
    "3. Train Word2Vec model on tokenized corpus\n",
    "4. Compute document embeddings (mean of token vectors)\n",
    "5. L2-normalize vectors for cosine similarity\n",
    "6. Save embeddings and metadata for downstream components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d546390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded | Seed set to 42\n",
      "✓ Data directories: Data/, Data/processed/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Component 1: Document embeddings with Word2Vec\n",
    "\n",
    "- Load movie plots\n",
    "- Tokenize (title + plot)\n",
    "- Train Word2Vec\n",
    "- Compute mean-pooled doc embeddings\n",
    "- L2-normalize\n",
    "- Save to Data/processed for Components 2–4\n",
    "\"\"\"\n",
    "\n",
    "# Setup: Imports, warnings, and reproducibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Directory paths (consistent across all components)\n",
    "DATA_DIR = \"Data\"\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Add Components directory to path\n",
    "sys.path.append('Components')\n",
    "\n",
    "# Download NLTK resources (required for tokenization)\n",
    "import nltk\n",
    "\n",
    "# Force download punkt_tab (required for NLTK 3.8+)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Also download punkt as fallback (for older NLTK versions)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Download stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Import Tokenizer and Word2Vec\n",
    "from Tokenizer import tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(\"✓ Imports loaded | Seed set to\", RANDOM_SEED)\n",
    "print(f\"✓ Data directories: {DATA_DIR}/, {PROCESSED_DIR}/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229019bc",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b890a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_movie_datasets(data_dir=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Load all movie datasets from CSV files.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the CSV files\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all movies and a 'decade' column\n",
    "    \"\"\"\n",
    "    decades = ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']\n",
    "    dataframes = []\n",
    "    \n",
    "    for decade in decades:\n",
    "        filepath = os.path.join(data_dir, f'{decade}-movies.csv')\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['decade'] = decade\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    all_movies = pd.concat(dataframes, ignore_index=True)\n",
    "    return all_movies\n",
    "\n",
    "\n",
    "def tokenize_documents(df, text_columns=['title', 'plot']):\n",
    "    \"\"\"\n",
    "    Tokenize documents by combining specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with movie data\n",
    "        text_columns: List of column names to combine for tokenization\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added 'tokens' column\n",
    "    \"\"\"\n",
    "    def combine_and_tokenize(row):\n",
    "        text = ' '.join([str(row[col]) for col in text_columns])\n",
    "        return tokenize(text, remove_stopwords=True, apply_stemming=True)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['tokens'] = df.apply(combine_and_tokenize, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_word2vec(sentences, vector_size=200, window=5, min_count=5, \n",
    "                   workers=4, sg=1, seed=42):\n",
    "    \"\"\"\n",
    "    Train Word2Vec model on tokenized sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of token lists\n",
    "        vector_size: Embedding dimension\n",
    "        window: Context window size\n",
    "        min_count: Minimum token frequency\n",
    "        workers: Number of worker threads\n",
    "        sg: Skip-gram (1) or CBOW (0)\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Trained Word2Vec model\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg,\n",
    "        seed=seed\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_document_embedding(tokens, model, vector_size):\n",
    "    \"\"\"\n",
    "    Compute document embedding as mean of token embeddings.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token strings\n",
    "        model: Trained Word2Vec model\n",
    "        vector_size: Embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        1D numpy array of shape (vector_size,)\n",
    "    \"\"\"\n",
    "    word_vectors = model.wv\n",
    "    valid_tokens = [t for t in tokens if t in word_vectors.key_to_index]\n",
    "    \n",
    "    if not valid_tokens:\n",
    "        return np.zeros(vector_size, dtype=np.float32)\n",
    "    \n",
    "    vecs = np.vstack([word_vectors[t] for t in valid_tokens])\n",
    "    return vecs.mean(axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_all_document_embeddings(df, model, vector_size):\n",
    "    \"\"\"\n",
    "    Compute embeddings for all documents.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'tokens' column\n",
    "        model: Trained Word2Vec model\n",
    "        vector_size: Embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        Document matrix of shape (n_docs, vector_size)\n",
    "    \"\"\"\n",
    "    doc_vectors = []\n",
    "    for tokens in df['tokens']:\n",
    "        doc_vec = compute_document_embedding(tokens, model, vector_size)\n",
    "        doc_vectors.append(doc_vec)\n",
    "    return np.vstack(doc_vectors)\n",
    "\n",
    "\n",
    "def normalize_vectors(doc_matrix):\n",
    "    \"\"\"\n",
    "    L2-normalize document vectors for cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        doc_matrix: Document matrix of shape (n_docs, dim)\n",
    "        \n",
    "    Returns:\n",
    "        Normalized document matrix\n",
    "    \"\"\"\n",
    "    norms = norm(doc_matrix, axis=1, keepdims=True)\n",
    "    norms[norms == 0.0] = 1.0  # Avoid division by zero\n",
    "    return doc_matrix / norms\n",
    "\n",
    "\n",
    "def save_embeddings(doc_matrix, df, output_dir=PROCESSED_DIR):\n",
    "    \"\"\"\n",
    "    Save document embeddings and metadata to disk.\n",
    "    \n",
    "    Args:\n",
    "        doc_matrix: Document matrix\n",
    "        df: DataFrame with metadata\n",
    "        output_dir: Output directory path\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (vector_path, metadata_path)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare metadata with doc_id\n",
    "    df_meta = df.copy().reset_index(drop=True)\n",
    "    df_meta['doc_id'] = df_meta.index\n",
    "    \n",
    "    # Save vectors\n",
    "    vector_path = os.path.join(output_dir, 'doc_vectors_w2v.npy')\n",
    "    np.save(vector_path, doc_matrix)\n",
    "    \n",
    "    # Save metadata with doc_id\n",
    "    metadata_path = os.path.join(output_dir, 'doc_metadata.csv')\n",
    "    df_meta[['doc_id', 'title', 'decade']].to_csv(metadata_path, index=False)\n",
    "    \n",
    "    return vector_path, metadata_path\n",
    "\n",
    "\n",
    "def show_similar_movies(target_idx, doc_matrix, df, top_k=5):\n",
    "    \"\"\"\n",
    "    Print the top-k most similar movies to a given movie index\n",
    "    using cosine similarity (dot product on normalized vectors).\n",
    "    \n",
    "    Args:\n",
    "        target_idx: Index of the query movie\n",
    "        doc_matrix: Normalized document matrix\n",
    "        df: DataFrame with movie metadata\n",
    "        top_k: Number of similar movies to return\n",
    "    \"\"\"\n",
    "    target_vec = doc_matrix[target_idx]\n",
    "    sims = doc_matrix @ target_vec  # cosine similarity (normalized vectors)\n",
    "    \n",
    "    # Get top-k most similar (excluding the query itself)\n",
    "    ranked = np.argsort(-sims)\n",
    "    ranked = ranked[ranked != target_idx][:top_k]\n",
    "    \n",
    "    print(f\"Query movie [{target_idx}]: {df.iloc[target_idx]['title']} ({df.iloc[target_idx]['decade']})\\n\")\n",
    "    print(\"Most similar movies:\")\n",
    "    for rank, idx in enumerate(ranked, start=1):\n",
    "        title = df.iloc[idx][\"title\"]\n",
    "        decade = df.iloc[idx][\"decade\"]\n",
    "        score = sims[idx]\n",
    "        print(f\"  {rank}. {title} ({decade})  |  sim = {score:.3f}\")\n",
    "\n",
    "print(\"Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73db11",
   "metadata": {},
   "source": [
    "## Component 1 – Design Notes\n",
    "\n",
    "**Tokenization.**  \n",
    "We use a custom tokenizer that lowercases text, removes stopwords and applies stemming (Porter). In classical IR this is a standard preprocessing pipeline to reduce sparsity and group morphological variants into a single term, which improves generalization for similarity search (Manning et al., *Introduction to Information Retrieval*, Ch. 2–3).\n",
    "\n",
    "**Dense document representations (Word2Vec).**  \n",
    "Instead of sparse TF–IDF vectors or LSI, we represent each document as the **average of its Word2Vec word embeddings**. Distributional word vectors (Mikolov et al., 2013) capture semantic similarity (synonyms, paraphrases), which is useful for \"similar movie plots\" retrieval: two plots can be close even if they do not share exactly the same words.\n",
    "\n",
    "**Why average pooling?**  \n",
    "Average pooling over word vectors is a simple, fast and surprisingly strong baseline for document embeddings. It keeps the representation size fixed (here 200-D), is easy to store and index, and works well in practice for downstream similarity tasks compared to heavier models (e.g. doc2vec, transformers) which would be overkill for this assignment.\n",
    "\n",
    "**Hyperparameters for Word2Vec.**  \n",
    "We set `vector_size=200`, `window=5`, `min_count=5`, `sg=1` (skip-gram).  \n",
    "- 200 dimensions are a common trade-off between expressiveness and memory/runtime.  \n",
    "- Window size 5 focuses on local context, which is appropriate for narrative movie plots.  \n",
    "- `min_count=5` filters out extremely rare words that would just add noise.  \n",
    "- Skip-gram (`sg=1`) typically works better on smaller datasets and for rare words than CBOW, which is helpful with ~17k movie plots.\n",
    "\n",
    "These choices are in line with typical Word2Vec setups reported in the literature (Mikolov et al., 2013).\n",
    "\n",
    "**Cosine similarity and L2-normalization.**  \n",
    "We L2-normalize all document vectors so that cosine similarity reduces to a dot product. Cosine similarity is standard for comparing embedding vectors, because it focuses on direction instead of magnitude and is more robust to document length differences (common IR practice).\n",
    "\n",
    "---\n",
    "\n",
    "## Main Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d518e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading movie datasets...\n",
      "  Loaded 17830 movies\n",
      "  Distribution: {'1970s': np.int64(1770), '1980s': np.int64(2338), '1990s': np.int64(3105), '2000s': np.int64(4416), '2010s': np.int64(4960), '2020s': np.int64(1241)}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load datasets\n",
    "print(\"Step 1: Loading movie datasets...\")\n",
    "all_movies = load_movie_datasets(data_dir=DATA_DIR)\n",
    "print(f\"  Loaded {len(all_movies)} movies\")\n",
    "print(f\"  Distribution: {dict(all_movies['decade'].value_counts().sort_index())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1b9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Tokenizing documents...\n",
      "✓ Tokenized 17830 documents\n",
      "  Total tokens: 4,577,616\n",
      "  Vocabulary size: 65,429 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize documents\n",
    "# We follow a classic IR pipeline: lowercase + stopword removal + stemming\n",
    "# → reduces sparsity and makes the later embedding model easier to train\n",
    "print(\"\\nStep 2: Tokenizing documents...\")\n",
    "all_movies = tokenize_documents(all_movies, text_columns=['title', 'plot'])\n",
    "\n",
    "total_tokens = sum(len(tokens) for tokens in all_movies['tokens'])\n",
    "vocabulary = set()\n",
    "for tokens in all_movies['tokens']:\n",
    "    vocabulary.update(tokens)\n",
    "\n",
    "print(f\"✓ Tokenized {len(all_movies)} documents\")\n",
    "print(f\"  Total tokens: {total_tokens:,}\")\n",
    "print(f\"  Vocabulary size: {len(vocabulary):,} unique tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7dc62",
   "metadata": {},
   "source": [
    "## Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31504eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Training Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Word2Vec model trained\n",
      "  Vocabulary size: 29,086 tokens\n",
      "  Embedding dimension: 200\n",
      "✓ Saved Word2Vec model: Data/processed/w2v_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train Word2Vec model\n",
    "# 200-D skip-gram is a common, strong baseline for semantic similarity\n",
    "print(\"\\nStep 3: Training Word2Vec model...\")\n",
    "\n",
    "# Hyperparameters\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW = 5\n",
    "MIN_COUNT = 5\n",
    "WORKERS = 4\n",
    "SG = 1  # Skip-gram\n",
    "\n",
    "sentences = list(all_movies['tokens'])\n",
    "w2v_model = train_word2vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS,\n",
    "    sg=SG,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"✓ Word2Vec model trained\")\n",
    "print(f\"  Vocabulary size: {len(w2v_model.wv.key_to_index):,} tokens\")\n",
    "print(f\"  Embedding dimension: {VECTOR_SIZE}\")\n",
    "\n",
    "# Save Word2Vec model for reproducibility and future use\n",
    "model_path = os.path.join(PROCESSED_DIR, 'w2v_model.bin')\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "w2v_model.save(model_path)\n",
    "print(f\"✓ Saved Word2Vec model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3471e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Computing document embeddings...\n",
      "✓ Document embeddings computed: shape (17830, 200)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compute document embeddings\n",
    "# Simple but effective document embedding: average all word vectors that appear in the movie plot\n",
    "print(\"\\nStep 4: Computing document embeddings...\")\n",
    "doc_matrix = compute_all_document_embeddings(all_movies, w2v_model, VECTOR_SIZE)\n",
    "print(f\" Document embeddings computed: shape {doc_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: L2-normalizing vectors for cosine similarity...\n",
      "Normalization complete\n",
      "  Example: norm before=1.5255, after=1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 5: L2-normalize vectors\n",
    "# Normalize so that later we can use cosine similarity / dot products directly\n",
    "print(\"\\nStep 5: L2-normalizing vectors for cosine similarity...\")\n",
    "doc_matrix_normalized = normalize_vectors(doc_matrix)\n",
    "print(f\"  Normalization complete\")\n",
    "print(f\"  Example: norm before={norm(doc_matrix[0]):.4f}, after={norm(doc_matrix_normalized[0]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cf0d4",
   "metadata": {},
   "source": [
    "### Dataset Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da098202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "  Total documents: 17830\n",
      "  Columns: ['title', 'image', 'plot', 'decade', 'tokens']\n",
      "  Missing values: 0\n",
      "\n",
      "Plot length statistics:\n",
      "  Mean: 2700 chars\n",
      "  Median: 2867 chars\n",
      "  Min: 3 chars\n",
      "  Max: 66145 chars\n"
     ]
    }
   ],
   "source": [
    "# Dataset overview\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"  Total documents: {len(all_movies)}\")\n",
    "print(f\"  Columns: {list(all_movies.columns)}\")\n",
    "print(f\"  Missing values: {all_movies.isnull().sum().sum()}\")\n",
    "\n",
    "# Plot length statistics\n",
    "all_movies['plot_length'] = all_movies['plot'].str.len()\n",
    "print(f\"\\nPlot length statistics:\")\n",
    "print(f\"  Mean: {all_movies['plot_length'].mean():.0f} chars\")\n",
    "print(f\"  Median: {all_movies['plot_length'].median():.0f} chars\")\n",
    "print(f\"  Min: {all_movies['plot_length'].min():.0f} chars\")\n",
    "print(f\"  Max: {all_movies['plot_length'].max():.0f} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5395cd0",
   "metadata": {},
   "source": [
    "### Tokenization Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65fa59e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count per document:\n",
      "  Mean: 256.7 tokens\n",
      "  Median: 271.0 tokens\n",
      "  Min: 4 tokens\n",
      "  Max: 6243 tokens\n",
      "\n",
      "Top 10 most frequent tokens:\n",
      "  kill                : 24,252 occurrences\n",
      "  find                : 22,815 occurrences\n",
      "  film                : 19,636 occurrences\n",
      "  take                : 18,517 occurrences\n",
      "  tell                : 17,799 occurrences\n",
      "  one                 : 17,778 occurrences\n",
      "  get                 : 17,059 occurrences\n",
      "  leav                : 16,972 occurrences\n",
      "  back                : 14,523 occurrences\n",
      "  return              : 13,432 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Token count per document\n",
    "all_movies['token_count'] = all_movies['tokens'].apply(len)\n",
    "print(\"Token count per document:\")\n",
    "print(f\"  Mean: {all_movies['token_count'].mean():.1f} tokens\")\n",
    "print(f\"  Median: {all_movies['token_count'].median():.1f} tokens\")\n",
    "print(f\"  Min: {all_movies['token_count'].min()} tokens\")\n",
    "print(f\"  Max: {all_movies['token_count'].max()} tokens\")\n",
    "\n",
    "# Most frequent tokens\n",
    "all_tokens_flat = [token for tokens in all_movies['tokens'] for token in tokens]\n",
    "token_freq = Counter(all_tokens_flat)\n",
    "print(f\"\\nTop 10 most frequent tokens:\")\n",
    "for token, count in token_freq.most_common(10):\n",
    "    print(f\"  {token:20s}: {count:6,} occurrences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1700ccb",
   "metadata": {},
   "source": [
    "### Sample Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c51170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized documents:\n",
      "\n",
      "1. 'Gator Bait (1970s)\n",
      "   Tokens (59): ['bait', 'film', 'follow', 'barefoot', 'poacher', 'name', 'desire', 'thibodeau', 'live', 'deep', 'swampland', 'ben', 'bracken', 'deputi', 'billi']...\n",
      "\n",
      "2. ...And Justice for All (film) (1970s)\n",
      "   Tokens (483): ['justic', 'film', 'arthur', 'kirkland', 'baltimor', 'defens', 'attorney', 'jail', 'contempt', 'court', 'charg', 'punch', 'judg', 'henri', 'fleme']...\n",
      "\n",
      "3. 10 (1979 film) (1970s)\n",
      "   Tokens (284): ['10', '1979', 'film', 'surpris', '42nd', 'birthday', 'parti', 'wealthi', 'famou', 'compos', 'georg', 'webber', 'thrown', 'actress', 'girlfriend']...\n"
     ]
    }
   ],
   "source": [
    "# Show sample tokenized documents\n",
    "print(\"Sample tokenized documents:\")\n",
    "for i in range(3):\n",
    "    movie = all_movies.iloc[i]\n",
    "    print(f\"\\n{i+1}. {movie['title']} ({movie['decade']})\")\n",
    "    print(f\"   Tokens ({len(movie['tokens'])}): {movie['tokens'][:15]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3eb8e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing semantic similarity with cosine distance:\n",
      "\n",
      "Query movie [0]: 'Gator Bait (1970s)\n",
      "\n",
      "Most similar movies:\n",
      "  1. Kill or Be Killed (2015 film) (2010s)  |  sim = 0.953\n",
      "  2. The Tripper (2000s)  |  sim = 0.949\n",
      "  3. Bloody Mama (1970s)  |  sim = 0.949\n",
      "  4. Lake Dead (2000s)  |  sim = 0.948\n",
      "  5. Sin City (film) (2000s)  |  sim = 0.948\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search on a few examples\n",
    "print(\"Testing semantic similarity with cosine distance:\\n\")\n",
    "show_similar_movies(0, doc_matrix_normalized, all_movies, top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
