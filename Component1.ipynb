{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d546390",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('Components')\n",
    "from Tokenizer import tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229019bc",
   "metadata": {},
   "source": [
    "## Loading Dataset and Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all movie datasets\n",
    "print(\"\\n[1/5] Loading datasets...\")\n",
    "dataframes = []\n",
    "for decade in ['1970s', '1980s', '1990s', '2000s', '2010s', '2020s']:\n",
    "    df = pd.read_csv(f'data/{decade}-movies.csv')\n",
    "    df['decade'] = decade\n",
    "    dataframes.append(df)\n",
    "    print(f\"  ✓ Loaded {len(df)} movies from {decade}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all movies\n",
    "all_movies = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"\\nTotal movies loaded: {len(all_movies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"\\n[2/5] Data Exploration...\")\n",
    "print(\"-\"*80)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"  - Columns: {list(all_movies.columns)}\")\n",
    "print(f\"  - Shape: {all_movies.shape}\")\n",
    "print(f\"  - Missing values: {all_movies.isnull().sum().to_dict()}\")\n",
    "\n",
    "print(\"\\n\\nFirst 3 movies:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(3):\n",
    "    movie = all_movies.iloc[i]\n",
    "    print(f\"\\n{i+1}. {movie['title']} ({movie['decade']})\")\n",
    "    plot_preview = movie['plot'][:150] + \"...\" if len(movie['plot']) > 150 else movie['plot']\n",
    "    print(f\"   Plot: {plot_preview}\")\n",
    "\n",
    "print(\"\\n\\nMovies per decade:\")\n",
    "print(all_movies['decade'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nPlot length statistics:\")\n",
    "all_movies['plot_length'] = all_movies['plot'].str.len()\n",
    "print(f\"  - Mean: {all_movies['plot_length'].mean():.0f} characters\")\n",
    "print(f\"  - Median: {all_movies['plot_length'].median():.0f} characters\")\n",
    "print(f\"  - Min: {all_movies['plot_length'].min():.0f} characters\")\n",
    "print(f\"  - Max: {all_movies['plot_length'].max():.0f} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7dc62",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31504eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all documents\n",
    "print(\"\\n[3/5] Tokenizing documents...\")\n",
    "print(\"Processing movie titles and plots...\")\n",
    "print(\"Using NLTK tokenization with stemming (Porter) for optimal IR performance...\")\n",
    "\n",
    "# Tokenize each movie (title + plot)\n",
    "# For indexing: use stemming and remove stopwords\n",
    "all_movies['tokens'] = all_movies.apply(\n",
    "    lambda row: tokenize(str(row['title']) + ' ' + str(row['plot']), \n",
    "                        remove_stopwords=True, \n",
    "                        apply_stemming=True),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3471e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total tokens\n",
    "total_tokens = sum(len(tokens) for tokens in all_movies['tokens'])\n",
    "print(f\"  ✓ Processed {len(all_movies)} documents\")\n",
    "print(f\"  ✓ Total tokens: {total_tokens:,}\")\n",
    "\n",
    "# Tokenization analysis\n",
    "print(\"\\n[4/5] Tokenization Analysis...\")\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token count per document\n",
    "all_movies['token_count'] = all_movies['tokens'].apply(len)\n",
    "print(f\"\\nTokens per document statistics:\")\n",
    "print(f\"  - Mean: {all_movies['token_count'].mean():.1f} tokens\")\n",
    "print(f\"  - Median: {all_movies['token_count'].median():.1f} tokens\")\n",
    "print(f\"  - Min: {all_movies['token_count'].min()} tokens\")\n",
    "print(f\"  - Max: {all_movies['token_count'].max()} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5472092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "print(f\"\\nBuilding vocabulary...\")\n",
    "vocabulary = set()\n",
    "for tokens in all_movies['tokens']:\n",
    "    vocabulary.update(tokens)\n",
    "print(f\"  ✓ Unique tokens in vocabulary: {len(vocabulary):,}\")\n",
    "\n",
    "# Most common tokens\n",
    "from collections import Counter\n",
    "all_tokens_flat = [token for tokens in all_movies['tokens'] for token in tokens]\n",
    "token_freq = Counter(all_tokens_flat)\n",
    "print(f\"\\nTop 20 most frequent tokens:\")\n",
    "for token, count in token_freq.most_common(20):\n",
    "    print(f\"  {token:20s} : {count:6,} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9599bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample\n",
    "print(\"\\n[5/5] Sample tokenized documents:\")\n",
    "print(\"-\"*80)\n",
    "for i in range(3):\n",
    "    movie = all_movies.iloc[i]\n",
    "    print(f\"\\n{i+1}. {movie['title']} ({movie['decade']})\")\n",
    "    print(f\"   Original plot length: {len(movie['plot'])} chars\")\n",
    "    print(f\"   Tokens ({len(movie['tokens'])}): {movie['tokens'][:15]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df66d4c",
   "metadata": {},
   "source": [
    "## To dense Vectors using World2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#World2Vec\n",
    "print(\"\\n[6/5] Converting tokens to dense vectors using Word2Vec...\")\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences=all_movies['tokens'], vector_size=100, window=5, min_count=2, workers=4)\n",
    "print(\"  ✓ Word2Vec model trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eddb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "doc_vectors = []\n",
    "zeros = np.zeros(w2v_model.vector_size, dtype=np.float32)\n",
    "for tokens in sentences:\n",
    "    vecs = [w2v_model.wv[t] for t in tokens if t in w2v_model.wv]\n",
    "    doc_vectors.append(np.mean(vecs, axis=0) if vecs else zeros)\n",
    "\n",
    "doc_matrix = np.vstack(doc_vectors)\n",
    "doc_matrix.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "informationretrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
